{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "draft_data = np.load('draft_data.npy')\n",
    "print(draft_data)\n",
    "\n",
    "mmr_column = draft_data[:, 1]  # Extract the second column\n",
    "# plt.hist(mmr_column, bins=10)  # Adjust the number of bins as needed\n",
    "# plt.title('Histogram of Second Column')\n",
    "# plt.xlabel('Values')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "matches_data = []\n",
    "all_drafts = []\n",
    "all_wins = []\n",
    "all_drafts_single_team = []\n",
    "for match_row in draft_data:\n",
    "    # print('match_row: ', match_row)\n",
    "    matches_data.append({\n",
    "        \"match_id\": match_row[-1],\n",
    "        \"radiant_win\": match_row[0],\n",
    "        \"radiant_team\": match_row[2:7],\n",
    "        \"dire_team\": match_row[7:12],\n",
    "        \"match_mmr\": match_row[1]\n",
    "    })\n",
    "    # print(match_row[2:7])\n",
    "    \n",
    "    # if match_row[0] == 1:\n",
    "    #     # all_drafts.append(match_row[2:7])\n",
    "    #     all_drafts.append(match_row[7:12])\n",
    "    # else:\n",
    "    #     all_drafts.append(match_row[2:7])\n",
    "    #     # all_drafts.append(match_row[7:12])\n",
    "\n",
    "    #radiant draft entry\n",
    "    # all_drafts.append(match_row[2:7])\n",
    "    # if match_row[0] == 1:\n",
    "    #     all_wins.append(1)\n",
    "    # else:\n",
    "    #     all_wins.append(0)\n",
    "        \n",
    "    # #dire draft entry\n",
    "    # all_drafts.append(match_row[7:12])\n",
    "    # if match_row[0] == 0:\n",
    "    #     all_wins.append(1)\n",
    "    # else:\n",
    "    #     all_wins.append(0)\n",
    "    \n",
    "    all_drafts_single_team.append(match_row[2:7])\n",
    "    all_drafts_single_team.append(match_row[7:12])\n",
    "        \n",
    "    #total draft entry\n",
    "    all_drafts.append(match_row[2:12])\n",
    "    all_wins.append(match_row[0])\n",
    "    \n",
    "# unique_numbers = np.unique(all_drafts)\n",
    "# print('number of diff heroes: ', len(unique_numbers))\n",
    "# print(unique_numbers)\n",
    "all_drafts = pd.DataFrame(all_drafts)\n",
    "all_drafts_single_team = pd.DataFrame(all_drafts_single_team)\n",
    "\n",
    "\n",
    "print(matches_data[0])\n",
    "print(all_drafts)\n",
    "print('size of all_drafts: ', all_drafts.size)\n",
    "print('all wins: ', all_wins)\n",
    "print('all wins size: ', len(all_wins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get(\"https://api.opendota.com/api/heroes?api_key=96eb7da5-ebee-4944-a6b9-61ca1127a989\")\n",
    "heroes_json = resp.json()\n",
    "# print(heroes_json)\n",
    "\n",
    "hero_id_to_name_dict = {}\n",
    "for hero in heroes_json:\n",
    "    hero_id_to_name_dict[hero['id']] = hero['localized_name']\n",
    "print(hero_id_to_name_dict[1])\n",
    "hero_id_to_name_dict[1] = 'Anti Mage'\n",
    "print(hero_id_to_name_dict)\n",
    "\n",
    "\n",
    "# Create the inverse dictionary\n",
    "name_to_hero_id_dict = {v: k for k, v in hero_id_to_name_dict.items()}\n",
    "\n",
    "# Print the first few items of the inverse dictionary\n",
    "for name, hero_id in list(name_to_hero_id_dict.items())[:10]:\n",
    "    print(\"Hero Name:\", name, \"- Hero ID:\", hero_id)\n",
    "\n",
    "def map_values(value):\n",
    "    return hero_id_to_name_dict.get(value, value)\n",
    "\n",
    "# Create a new DataFrame with mapped values\n",
    "all_drafts_names = all_drafts.applymap(lambda x: map_values(x))\n",
    "print(\"\\nDataFrame with mapped values:\")\n",
    "print(all_drafts_names)\n",
    "\n",
    "all_drafts_single_team_names = all_drafts_single_team.applymap(lambda x: map_values(x))\n",
    "print(\"\\nDataFrame with mapped values:\")\n",
    "print(all_drafts_single_team_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.linalg import triu\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print('size of draft df: ', all_drafts_single_team_names[0].size)\n",
    "# Step 1: Prepare the data (Assuming 'team_drafts' is your DataFrame)\n",
    "# team_drafts should contain one column for each hero in a team draft\n",
    "# Each row represents a single team draft, with heroes listed as columns\n",
    "\n",
    "# Step 2: Train the hero vectors\n",
    "model = Word2Vec(sentences=all_drafts_single_team_names.values.tolist(), vector_size=50, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Step 3: Retrieve the hero vectors\n",
    "hero_vectors_dict = {hero: model.wv[hero] for hero in model.wv.index_to_key}\n",
    "print('hero vectors dict: ', hero_vectors_dict)\n",
    "\n",
    "# Step 4: Find similar heroes\n",
    "def find_similar_heroes(hero, top_n=5):\n",
    "    similar_heroes = model.wv.most_similar(hero, topn=top_n)\n",
    "    return similar_heroes\n",
    "\n",
    "# Step 5: Perform analogies\n",
    "def perform_analogy(hero_a, hero_b, hero_c, top_n=5):\n",
    "    try:\n",
    "        analogy = model.wv.most_similar(positive=[hero_b, hero_c], negative=[hero_a], topn=top_n)\n",
    "        return analogy\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "shadow_fiend_similar_heroes = find_similar_heroes(\"Shadow Fiend\")\n",
    "print(\"Similar heroes to Shadow Fiend:\")\n",
    "for hero, similarity in shadow_fiend_similar_heroes:\n",
    "    print(hero, similarity)\n",
    "    \n",
    "shadow_fiend_similar_heroes = find_similar_heroes(\"Phoenix\")\n",
    "print(\"\\nSimilar heroes to Phoenix:\")\n",
    "for hero, similarity in shadow_fiend_similar_heroes:\n",
    "    print(hero, similarity)\n",
    "\n",
    "lion_antimage_analogy = perform_analogy(\"Lion\", \"Anti Mage\", \"Witch Doctor\")\n",
    "print(\"\\nAnalogies:\")\n",
    "for hero, similarity in lion_antimage_analogy:\n",
    "    print(hero, similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Extract hero vectors\n",
    "hero_vectors = np.array([model.wv[hero] for hero in model.wv.index_to_key])\n",
    "print(\"hero vectors: \", hero_vectors)\n",
    "\n",
    "# Step 2: Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "hero_vectors_2d = tsne.fit_transform(hero_vectors)\n",
    "\n",
    "#Step 4: Plot the 2D projection of hero vectors with hero names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(hero_vectors_2d[:, 0], hero_vectors_2d[:, 1])#, c=colors)\n",
    "\n",
    "# Annotate each point with hero name\n",
    "for i, hero in enumerate(model.wv.index_to_key):\n",
    "    plt.annotate(hero, (hero_vectors_2d[i, 0], hero_vectors_2d[i, 1]), fontsize=6)\n",
    "\n",
    "plt.title(\"2-Dimensional Visualization of Hero Vectors from Word2Vec\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hero_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming hero_features is a DataFrame containing the 50 non-concrete features for each hero\n",
    "# hero_features.shape should be (120, 50)\n",
    "# hero_features = hero_vectors\n",
    "hero_names = list(hero_vectors_dict.keys())\n",
    "hero_vectors = np.array(list(hero_vectors_dict.values()))\n",
    "# print(hero_names)\n",
    "# print(hero_vectors)\n",
    "\n",
    "# Define the number of clusters for each layer (larger numbers mean smaller clusters)\n",
    "num_clusters_layer1 = 40\n",
    "num_clusters_layer2 = 30\n",
    "num_clusters_layer3 = 22\n",
    "num_clusters_layer4 = 14\n",
    "num_clusters_layer5 = 9\n",
    "num_clusters_layer6 = 5\n",
    "\n",
    "# Perform clustering for each layer\n",
    "kmeans_layer1 = KMeans(n_clusters=num_clusters_layer1, random_state=3435).fit(hero_vectors)\n",
    "kmeans_layer2 = KMeans(n_clusters=num_clusters_layer2, random_state=3435).fit(hero_vectors)\n",
    "kmeans_layer3 = KMeans(n_clusters=num_clusters_layer3, random_state=3435).fit(hero_vectors)\n",
    "kmeans_layer4 = KMeans(n_clusters=num_clusters_layer4, random_state=3435).fit(hero_vectors)\n",
    "kmeans_layer5 = KMeans(n_clusters=num_clusters_layer5, random_state=3435).fit(hero_vectors)\n",
    "kmeans_layer6 = KMeans(n_clusters=num_clusters_layer6, random_state=3435).fit(hero_vectors)\n",
    "\n",
    "# Get cluster assignments for each hero\n",
    "hero_cluster_layer1 = kmeans_layer1.labels_\n",
    "hero_cluster_layer2 = kmeans_layer2.labels_\n",
    "hero_cluster_layer3 = kmeans_layer3.labels_\n",
    "hero_cluster_layer4 = kmeans_layer4.labels_\n",
    "hero_cluster_layer5 = kmeans_layer5.labels_\n",
    "hero_cluster_layer6 = kmeans_layer6.labels_\n",
    "clusters = [hero_cluster_layer1, hero_cluster_layer2, hero_cluster_layer3, hero_cluster_layer4, hero_cluster_layer5, hero_cluster_layer6]\n",
    "\n",
    "# print(\"cluster layer 1: \", hero_cluster_layer1)\n",
    "# print(\"cl1 length: \", len(hero_cluster_layer1))\n",
    "# print(\"cluster layer 2: \", hero_cluster_layer2)\n",
    "# print(\"cluster layer 3: \", hero_cluster_layer3)\n",
    "\n",
    "# Create a dictionary mapping each hero name to the clusters they belong to\n",
    "hero_clusters_dict = {}\n",
    "for hero_name, cluster1, cluster2, cluster3, cluster4, cluster5, cluster6 in zip(hero_names, hero_cluster_layer1, hero_cluster_layer2, hero_cluster_layer3, hero_cluster_layer4, hero_cluster_layer5, hero_cluster_layer6):\n",
    "    hero_clusters_dict[hero_name] = [cluster6, cluster5, cluster4, cluster3, cluster2, cluster1]\n",
    "\n",
    "# Assuming hero_vectors contains vectors for all heroes and clusters contains clusters\n",
    "phoenix_cluster_level1 = None\n",
    "phoenix_cluster_level2 = None\n",
    "phoenix_cluster_level3 = None\n",
    "phoenix_cluster_level4 = None\n",
    "phoenix_cluster_level5 = None\n",
    "phoenix_cluster_level6 = None\n",
    "\n",
    "# print(\"hero vectors: \", hero_vectors)\n",
    "hero_list = model.wv.index_to_key\n",
    "print(\"hero list: \", hero_list)\n",
    "# phoenix_index = np.where(hero_list == \"Phoenix\")[0]\n",
    "phoenix_index = hero_list.index(\"Phoenix\")\n",
    "# print(phoenix_index)\n",
    "# Assuming clusters are indexed by their level: lowest, middle, highest\n",
    "level = 1\n",
    "for cluster in clusters:    \n",
    "    phoenix_cluster = cluster[phoenix_index]\n",
    "    indices = [i for i in range(len(cluster)) if cluster[i] == phoenix_cluster]\n",
    "    # print(indices)\n",
    "    \n",
    "    if level == 1:\n",
    "        phoenix_cluster_1 = indices\n",
    "        print('\\n', phoenix_cluster_1)\n",
    "    elif level == 2:\n",
    "        phoenix_cluster_2 = indices\n",
    "        print(phoenix_cluster_2)\n",
    "    elif level == 3:\n",
    "        phoenix_cluster_3 = indices\n",
    "        print(phoenix_cluster_3)\n",
    "    elif level == 4:\n",
    "        phoenix_cluster_4 = indices\n",
    "        print(phoenix_cluster_4)\n",
    "    elif level == 5:\n",
    "        phoenix_cluster_5 = indices\n",
    "        print(phoenix_cluster_5)\n",
    "    elif level == 6:\n",
    "        phoenix_cluster_6 = indices\n",
    "        print(phoenix_cluster_6,'\\n')\n",
    "    level += 1\n",
    "\n",
    "def display_loop(phoenix_cluster_x):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(hero_vectors_2d[:, 0], hero_vectors_2d[:, 1])\n",
    "\n",
    "    # Annotate each point with hero name\n",
    "    for i, hero in enumerate(model.wv.index_to_key):\n",
    "        if hero in [hero_list[j] for j in phoenix_cluster_x]:\n",
    "            plt.annotate(hero, (hero_vectors_2d[i, 0], hero_vectors_2d[i, 1]), fontsize=6)\n",
    "\n",
    "    plt.title(\"2-Dimensional Visualization of Hero Vectors from Word2Vec\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Heroes in Phoenix's 1st layer:\",[hero_list[i] for i in phoenix_cluster_1])\n",
    "display_loop(phoenix_cluster_1)\n",
    "print(\"Heroes in Phoenix's 2nd layer:\", [hero_list[i] for i in phoenix_cluster_2])\n",
    "display_loop(phoenix_cluster_2)\n",
    "print(\"Heroes in Phoenix's 3rd layer:\", [hero_list[i] for i in phoenix_cluster_3])\n",
    "display_loop(phoenix_cluster_3)\n",
    "print(\"Heroes in Phoenix's 4th layer:\",[hero_list[i] for i in phoenix_cluster_4])\n",
    "display_loop(phoenix_cluster_4)\n",
    "print(\"Heroes in Phoenix's 5th layer:\", [hero_list[i] for i in phoenix_cluster_5])\n",
    "display_loop(phoenix_cluster_5)\n",
    "print(\"Heroes in Phoenix's 6th layer:\", [hero_list[i] for i in phoenix_cluster_6])\n",
    "display_loop(phoenix_cluster_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Define a function to visualize clusters for a single layer\n",
    "def visualize_clusters(hero_vectors, cluster_assignments, title):\n",
    "    # Apply t-SNE to reduce dimensions to 2\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_result = tsne.fit_transform(hero_vectors)\n",
    "\n",
    "    # Get unique cluster assignments\n",
    "    unique_clusters = np.unique(cluster_assignments)\n",
    "\n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for cluster in unique_clusters:\n",
    "        indices = np.where(cluster_assignments == cluster)[0]\n",
    "        plt.scatter(tsne_result[indices, 0], tsne_result[indices, 1], label=f'Cluster {cluster}')\n",
    "        for i, hero in enumerate(model.wv.index_to_key):\n",
    "            if hero == \"Phoenix\" or hero == 'Tinker' or hero == 'Shadow Demon' or hero == 'Tusk' or hero == 'Ursa' or hero == 'Dawnreaker' or hero == \"Tidehunter\" or hero == \"Dazzle\":\n",
    "                plt.annotate(hero, (hero_vectors_2d[i, 0], hero_vectors_2d[i, 1]), fontsize=8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend(fontsize=\"5\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize clusters for each layer\n",
    "visualize_clusters(hero_vectors, hero_cluster_layer1, title='Layer 1 Clusters')\n",
    "visualize_clusters(hero_vectors, hero_cluster_layer2, title='Layer 2 Clusters')\n",
    "visualize_clusters(hero_vectors, hero_cluster_layer3, title='Layer 3 Clusters')\n",
    "visualize_clusters(hero_vectors, hero_cluster_layer4, title='Layer 4 Clusters')\n",
    "visualize_clusters(hero_vectors, hero_cluster_layer5, title='Layer 5 Clusters')\n",
    "visualize_clusters(hero_vectors, hero_cluster_layer6, title='Layer 6 Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample of Phoenix's hero_vector, 1 closely related hero, 1 slightly related hero, and 1 very different hero\")\n",
    "\n",
    "phoenix_index = hero_list.index(\"Phoenix\")\n",
    "sd_index = hero_list.index(\"Shadow Demon\")\n",
    "tusk_index = hero_list.index(\"Tusk\")\n",
    "spectre_index = hero_list.index(\"Spectre\")\n",
    "tinker_index = hero_list.index(\"Tinker\")\n",
    "\n",
    "#record vectors earlier and directly access them for accuracy\n",
    "phoenix_vector = hero_vectors[phoenix_index]\n",
    "# print('shape: ', phoenix_vector.shape)\n",
    "# print(phoenix_vector)\n",
    "tinker_vector = hero_vectors[tinker_index]\n",
    "sd_vector = hero_vectors[sd_index]\n",
    "tusk_vector = hero_vectors[tusk_index]\n",
    "spectre_vector = hero_vectors[spectre_index]\n",
    "\n",
    "eight_hero_vectors = np.vstack((phoenix_vector, sd_vector, phoenix_vector, tusk_vector, phoenix_vector, spectre_vector, phoenix_vector, tinker_vector))\n",
    "hero_names = ['Phoenix', 'Shadow Demon', 'Phoenix', 'Tusk', 'Phoenix', 'Spectre', 'Phoenix', 'Tinker']\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(eight_hero_vectors, cmap='viridis', xticklabels=False)\n",
    "plt.yticks(ticks=np.arange(8) + 0.5, labels=hero_names, rotation=0)\n",
    "plt.xlabel('NLP Feature Index')\n",
    "plt.ylabel('Hero')\n",
    "plt.title('Hero Vectors Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distances = np.linalg.norm(hero_vectors[:, np.newaxis] - hero_vectors[np.newaxis, :], axis=-1)\n",
    "np.fill_diagonal(pairwise_distances, np.inf)\n",
    "\n",
    "min_indices = np.unravel_index(np.argmin(pairwise_distances), pairwise_distances.shape)\n",
    "closest_pair = min_indices[0], min_indices[1]\n",
    "\n",
    "# Get the closest vectors\n",
    "closest_hero1 = hero_list[closest_pair[0]]\n",
    "closest_hero2 = hero_list[closest_pair[1]]\n",
    "\n",
    "print(\"Closest pair of Heroes:\")\n",
    "print(\"Hero 1:\", closest_hero1)\n",
    "print(\"Hero 2:\", closest_hero2)\n",
    "\n",
    "hero1_index = min_indices[0]\n",
    "hero2_index = min_indices[1]\n",
    "\n",
    "hero1_vector = hero_vectors[hero1_index]\n",
    "hero2_vector = hero_vectors[hero2_index]\n",
    "\n",
    "two_hero_vectors = np.vstack((hero1_vector, hero2_vector))\n",
    "hero_names = [closest_hero1, closest_hero2]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "sns.heatmap(two_hero_vectors, cmap='viridis', xticklabels=False)\n",
    "plt.yticks(ticks=np.arange(2) + 0.5, labels=hero_names, rotation=0)\n",
    "plt.xlabel('NLP Feature Index')\n",
    "plt.ylabel('Hero')\n",
    "plt.title('Two most similar (perhaps swappable!) heroes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if results are unsatisfactory, I'll switch to predicting off heroes on both teams\n",
    "#Preparing features\n",
    "training_data = []\n",
    "# hero_list = model.wv.index_to_key\n",
    "\n",
    "# print('all_drafts: ', all_drafts)\n",
    "# print('all_drafts type: ', type(all_drafts))\n",
    "\n",
    "for index, row in all_drafts_names.iterrows():\n",
    "    # print(index)\n",
    "    draft_list_of_cluster_lists = []\n",
    "    # print('row: ', row)\n",
    "    #form the 5x3 array of clusters for each hero\n",
    "    for hero_name in row:\n",
    "        # hero_name = hero_list[hero_id]\n",
    "        \n",
    "        list_of_clusters = []\n",
    "        \n",
    "        #translate hero name back into id\n",
    "        hero_id = name_to_hero_id_dict[hero_name]\n",
    "        list_of_clusters.append(hero_id)\n",
    "        \n",
    "        # hero_clusters = hero_clusters_dict[hero_name]\n",
    "        # # print('hero clusters: ', hero_clusters)\n",
    "        # list_of_clusters.append(hero_clusters[0])\n",
    "        # list_of_clusters.append(hero_clusters[1])\n",
    "        # list_of_clusters.append(hero_clusters[2])\n",
    "        \n",
    "        hero_clusters = hero_clusters_dict[hero_name]\n",
    "        print(\"hero clusters: \", hero_clusters)\n",
    "        list_of_clusters.extend(hero_clusters)\n",
    "        \n",
    "        draft_list_of_cluster_lists.append(list_of_clusters)\n",
    "        \n",
    "    training_data.append(draft_list_of_cluster_lists)\n",
    "\n",
    "print('first draft features: ', list(all_drafts.iloc[0]))\n",
    "print('training data size: ', len(training_data))\n",
    "print('upgraded first draft features: ', training_data[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get(\"https://api.opendota.com/api/heroStats?api_key=96eb7da5-ebee-4944-a6b9-61ca1127a989\")\n",
    "hero_stats = resp.json()\n",
    "print(hero_stats)\n",
    "\n",
    "#Preparing features\n",
    "training_data = []\n",
    "for index, row in all_drafts_names.iterrows():\n",
    "    draft_list_of_cluster_lists = []\n",
    "    for hero_name in row:        \n",
    "        list_of_clusters = []\n",
    "        \n",
    "        #translate hero name back into id\n",
    "        hero_id = name_to_hero_id_dict[hero_name]\n",
    "        list_of_clusters.append(hero_id)\n",
    "                \n",
    "        hero_clusters = hero_clusters_dict[hero_name]\n",
    "        # print(\"hero clusters: \", hero_clusters)\n",
    "        list_of_clusters.extend(hero_clusters)\n",
    "        \n",
    "        for d in hero_stats:\n",
    "\n",
    "            # Check if the current dictionary has the key-value pair 'id': 1\n",
    "            if d.get('id') == hero_id:\n",
    "                specific_hero_all_stats = d\n",
    "                # print(d['roles'])\n",
    "        # specific_hero_all_stats = hero_stats[hero_id]\n",
    "        # relevant_hero_stats = []\n",
    "        list_of_clusters.extend([specific_hero_all_stats['attack_type']])\n",
    "        list_of_clusters.extend([specific_hero_all_stats['primary_attr']])\n",
    "        list_of_clusters.extend(specific_hero_all_stats['roles'])\n",
    "        #specific_hero_all_stats[], specific_hero_all_stats[], specific_hero_all_stats[], specific_hero_all_stats[],\n",
    "        #specific_hero_all_stats[], specific_hero_all_stats[], specific_hero_all_stats[], specific_hero_all_stats[])\n",
    "        # print(\"relevant hero stats: \", relevant_hero_stats)\n",
    "        # list_of_clusters.extend(relevant_hero_stats)\n",
    "        \n",
    "        draft_list_of_cluster_lists.append(list_of_clusters)\n",
    "        \n",
    "    training_data.append(draft_list_of_cluster_lists)\n",
    "\n",
    "print('first draft features: ', list(all_drafts.iloc[0]))\n",
    "print('training data size: ', len(training_data))\n",
    "print('upgraded first draft features: ', training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize a dictionary to store the mapping of each unique categorical value to an integer index\n",
    "categorical_value_to_index = {}\n",
    "\n",
    "# Initialize a variable to store the maximum categorical length\n",
    "max_categorical_length = 0\n",
    "\n",
    "# Iterate over all data points to build the mapping dictionary and find the maximum categorical length\n",
    "for data_point in training_data:\n",
    "    for feature_list in data_point:\n",
    "        categorical_features = np.array(feature_list)[7:]\n",
    "        max_categorical_length = max(max_categorical_length, len(categorical_features))\n",
    "        for value in categorical_features:\n",
    "            if value not in categorical_value_to_index:\n",
    "                categorical_value_to_index[value] = len(categorical_value_to_index)\n",
    "\n",
    "# Initialize lists to store encoded categorical features and numerical features for each data point\n",
    "encoded_features_list = []\n",
    "numerical_features_list = []\n",
    "\n",
    "# Iterate over each data point\n",
    "for data_point in training_data:\n",
    "    # Initialize lists to store encoded categorical features and numerical features for this data point\n",
    "    encoded_features_data_point = []\n",
    "    numerical_features_data_point = []\n",
    "    \n",
    "    # Iterate over each list in the data point\n",
    "    for feature_list in data_point:\n",
    "        # Extract numerical features\n",
    "        numerical_features = np.array(feature_list)[:7]\n",
    "        \n",
    "        # Append the numerical features for this list to the data point list\n",
    "        numerical_features_data_point.append(numerical_features)\n",
    "        \n",
    "        # Extract categorical features\n",
    "        categorical_features = np.array(feature_list)[7:]\n",
    "        \n",
    "        # Convert categorical features to integers representing the index of each unique value\n",
    "        encoded_features = np.array([categorical_value_to_index.get(value, 0) for value in categorical_features])\n",
    "        \n",
    "        # Pad the encoded features with zeros to match the maximum categorical length\n",
    "        padded_encoded_features = np.pad(encoded_features, (0, max_categorical_length - len(categorical_features)), constant_values=0)\n",
    "        \n",
    "        # Append the padded encoded features for this list to the data point list\n",
    "        encoded_features_data_point.append(padded_encoded_features)\n",
    "    \n",
    "    # Stack the encoded features vertically for this data point\n",
    "    encoded_features_data_point = np.vstack(encoded_features_data_point)\n",
    "    \n",
    "    # Stack the numerical features vertically for this data point\n",
    "    numerical_features_data_point = np.vstack(numerical_features_data_point)\n",
    "    \n",
    "    # Append the encoded and numerical features for this data point to the respective lists\n",
    "    encoded_features_list.append(encoded_features_data_point)\n",
    "    numerical_features_list.append(numerical_features_data_point)\n",
    "\n",
    "# Find the maximum number of columns for both encoded and numerical features\n",
    "max_encoded_columns = max(encoded_feature.shape[1] for encoded_feature in encoded_features_list)\n",
    "max_numerical_columns = max(numerical_feature.shape[1] for numerical_feature in numerical_features_list)\n",
    "\n",
    "# Pad the encoded features with zeros to match the maximum number of columns\n",
    "padded_encoded_features_list = [np.pad(encoded_feature, ((0, 0), (0, max_encoded_columns - encoded_feature.shape[1])), constant_values=0) for encoded_feature in encoded_features_list]\n",
    "\n",
    "# Concatenate encoded and numerical features for all data points\n",
    "encoded_training_data = np.concatenate((numerical_features_list, padded_encoded_features_list), axis=2)\n",
    "\n",
    "# Print the processed data\n",
    "print(encoded_training_data)\n",
    "print(encoded_training_data.shape)\n",
    "\n",
    "encoded_training_data = np.array(encoded_training_data, dtype=int)\n",
    "print(encoded_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_value_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming draft_data contains the draft data with hero clusters and game results\n",
    "\n",
    "# Prepare the training data\n",
    "# X = training_data\n",
    "X = encoded_training_data #10 GROUPS of: 1 hero id, 6 clusters, 8 hero traits (including zero padding for heroes without)\n",
    "y = all_wins  # Target variable (game result: 0 for loss, 1 for win)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=3435)\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(10, 15)),  # Flatten the input shape\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    min_delta=0.001,  # Minimum change in monitored quantity to qualify as improvement\n",
    "    patience=7,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,  # Print message when training stops due to early stopping\n",
    "    mode='auto',  # Whether to minimize or maximize the monitored quantity ('auto' determines based on metric)\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Get training/validation accuracy\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot training/validation accuracy\n",
    "plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')\n",
    "\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = np.array(encoded_training_data)\n",
    "y = np.array(all_wins)\n",
    "\n",
    "# Split the data for each team\n",
    "X_team1 = X[:, 0:5, :]\n",
    "X_team2 = X[:, 5:10, :]\n",
    "\n",
    "# Split the data into training and testing sets for each team\n",
    "X_team1_train, X_team1_test, X_team2_train, X_team2_test, y_train, y_test = train_test_split(\n",
    "    X_team1, X_team2, y, test_size=0.15, random_state=3435,# shuffle = True\n",
    ")\n",
    "\n",
    "print('shape of x team 1: ', X_team1_train.shape)\n",
    "print('shape of x team 2: ', X_team2_train.shape)\n",
    "print('shape of y: ', y_train.shape)\n",
    "\n",
    "# Save the datasets to .npy files\n",
    "\n",
    "np.save('/Users/moengke/Desktop/CDA/DotA - Course Project/X_team1_train.npy', X_team1_train)\n",
    "np.save('/Users/moengke/Desktop/CDA/DotA - Course Project/X_team1_test.npy', X_team1_test)\n",
    "np.save('/Users/moengke/Desktop/CDA/DotA - Course Project/X_team2_train.npy', X_team2_train)\n",
    "np.save('/Users/moengke/Desktop/CDA/DotA - Course Project/X_team2_test.npy', X_team2_test)\n",
    "np.save('/Users/moengke/Desktop/CDA/DotA - Course Project/y_train.npy', y_train)\n",
    "np.save('/Users/moengke/Desktop/CDA/DotA - Course Project/y_test.npy', y_test)\n",
    "print('Files saved successfully.')\n",
    "\n",
    "X_team1_train = 'poop'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "base_path = '/Users/moengke/Desktop/CDA/DotA - Course Project/'\n",
    "\n",
    "X_team1_train = np.load(base_path + 'X_team1_train.npy')\n",
    "X_team1_test = np.load(base_path + 'X_team1_test.npy')\n",
    "X_team2_train = np.load(base_path + 'X_team2_train.npy')\n",
    "X_team2_test = np.load(base_path + 'X_team2_test.npy')\n",
    "y_train = np.load(base_path + 'y_train.npy')\n",
    "y_test = np.load(base_path + 'y_test.npy')\n",
    "\n",
    "print(X_team1_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Permute, Concatenate, Dense, Flatten\n",
    "import tqdm\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class CustomVerbose(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f'Starting Epoch {epoch+1}')\n",
    "        self.batch = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch += 1\n",
    "        if self.batch % 100 == 0:  # Adjust this to change how often you see updates\n",
    "            print(f'Batch {self.batch}, Loss: {logs[\"loss\"]}')\n",
    "\n",
    "class TQDMProgressBar(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epochs = getattr(self, 'epochs', 0) + 1\n",
    "        self.prog_bar = tqdm.tqdm(total=100, desc=f'Epoch {epoch + 1}/{self.epochs}')\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.prog_bar.update((1 / self.params['steps']) * 100)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.prog_bar.close()\n",
    "\n",
    "# MLP for each team\n",
    "def create_mlp():\n",
    "    input_layer = Input(shape=(5, 15))  # Each sample has a shape of 5x15\n",
    "    x = Flatten()(input_layer)  # Flatten the 5x15 matrix to a vector of 75 elements\n",
    "    x = Dense(8, activation='relu')(x)  # First dense layer\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(4, activation='relu')(x)  # Second dense layer\n",
    "    output = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "    return Model(inputs=input_layer, outputs=output)\n",
    "# MLP for both teams combined\n",
    "def create_combined_mlp():\n",
    "    # The shape (10, 15) reflects the concatenated teams data:\n",
    "    # 10 entities (e.g., heroes) each with 15 features\n",
    "    input_layer = Input(shape=(10, 15))\n",
    "    x = Flatten()(input_layer)  # Flatten the 10x15 matrix to a vector of 150 elements\n",
    "    x = Dense(8, activation='relu')(x)  # First dense layer with more neurons\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(4, activation='relu')(x)  # Second dense layer\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "\n",
    "base_path = '/Users/moengke/Desktop/CDA/DotA - Course Project/'\n",
    "X_team1_train = np.load(base_path + 'X_team1_train.npy')\n",
    "X_team1_test = np.load(base_path + 'X_team1_test.npy')\n",
    "X_team2_train = np.load(base_path + 'X_team2_train.npy')\n",
    "X_team2_test = np.load(base_path + 'X_team2_test.npy')\n",
    "y_train = np.load(base_path + 'y_train.npy')\n",
    "y_test = np.load(base_path + 'y_test.npy')\n",
    "# Combine the team data for the dual team MLP\n",
    "X_combined_train = np.concatenate([X_team1_train, X_team2_train], axis=1)\n",
    "X_combined_test = np.concatenate([X_team1_test, X_team2_test], axis=1)\n",
    "print(X_team1_train.shape)\n",
    "\n",
    "\n",
    "# Instantiate MLPs\n",
    "team1_mlp = create_mlp()\n",
    "team2_mlp = create_mlp()\n",
    "combined_mlp = create_combined_mlp()\n",
    "# Inputs\n",
    "team1_input = Input(shape=(5, 15))\n",
    "team2_input = Input(shape=(5, 15))\n",
    "combined_input = Input(shape=(10, 15))\n",
    "# Get MLP outputs\n",
    "team1_output = team1_mlp(team1_input)\n",
    "team2_output = team2_mlp(team2_input)\n",
    "combined_output = combined_mlp(combined_input)\n",
    "# Concatenate all MLP outputs\n",
    "concatenated = Concatenate()([team1_output, team2_output, combined_output])\n",
    "# Final MLP for prediction\n",
    "x = Dense(8, activation='relu')(concatenated)\n",
    "x = Dense(4, activation='relu')(x)\n",
    "final_output = Dense(1, activation='sigmoid')(x)\n",
    "# Final model\n",
    "model = Model(inputs=[team1_input, team2_input, combined_input], outputs=final_output)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=4, verbose=1, mode='auto', restore_best_weights=True)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_team1_train, X_team2_train, X_combined_train],\n",
    "    y_train,\n",
    "    validation_split=0.1,  # Using 10% of the training data for validation\n",
    "    epochs=50,\n",
    "    #batch_size=4,\n",
    "    callbacks=[early_stopping, TQDMProgressBar(), CustomVerbose()]\n",
    ")\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [X_team1_test, X_team2_test, X_combined_test],\n",
    "    y_test\n",
    ")\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from spektral.layers import GCNConv\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "base_path = '/Users/moengke/Desktop/CDA/DotA - Course Project/'\n",
    "X_team1_train = np.load(base_path + 'X_team1_train.npy')\n",
    "X_team1_test = np.load(base_path + 'X_team1_test.npy')\n",
    "X_team2_train = np.load(base_path + 'X_team2_train.npy')\n",
    "X_team2_test = np.load(base_path + 'X_team2_test.npy')\n",
    "y_train = np.load(base_path + 'y_train.npy')\n",
    "y_test = np.load(base_path + 'y_test.npy')\n",
    "\n",
    "# Define the model\n",
    "def create_gnn_branch(input_shape, adjacency_input, features_input, output_units=32):\n",
    "    feat_input = Input(shape=(None, input_shape), name=f'features_input_{features_input}')\n",
    "    adj_input = Input(shape=(None, None), sparse=True, name=f'adjacency_input_{adjacency_input}')\n",
    "    gc1 = GCNConv(64, activation='relu')([feat_input, adj_input])\n",
    "    gc2 = GCNConv(output_units, activation='relu')([gc1, adj_input])\n",
    "    pool = GlobalAveragePooling1D()(gc2)\n",
    "    return Model(inputs=[feat_input, adj_input], outputs=pool, name=f'gnn_branch_{features_input}')\n",
    "\n",
    "def create_full_connected_adjacency(num_nodes):\n",
    "    \"\"\"Create a fully connected adjacency matrix.\"\"\"\n",
    "    # Creating a matrix where each node is connected to every other node\n",
    "    adjacency = np.ones((num_nodes, num_nodes)) - np.eye(num_nodes)\n",
    "    return adjacency\n",
    "\n",
    "# Number of heroes per team\n",
    "num_heroes_per_team = 5\n",
    "\n",
    "# Create fully connected adjacency matrices for each team\n",
    "A1 = create_full_connected_adjacency(num_heroes_per_team)\n",
    "A2 = create_full_connected_adjacency(num_heroes_per_team)\n",
    "\n",
    "# Create a combined fully connected adjacency matrix for all heroes (10 heroes)\n",
    "num_heroes_combined = num_heroes_per_team * 2\n",
    "A_combined = create_full_connected_adjacency(num_heroes_combined)\n",
    "\n",
    "# Create GNN branches\n",
    "gnn_branch_1 = create_gnn_branch(total_traits, 'A1', 'team_1')\n",
    "gnn_branch_2 = create_gnn_branch(total_traits, 'A2', 'team_2')\n",
    "gnn_combined = create_gnn_branch(total_traits, 'A_combined', 'combined')\n",
    "\n",
    "# Input layers\n",
    "features_team_1 = Input(shape=(num_heroes_per_team, total_traits), name='features_team_1')\n",
    "features_team_2 = Input(shape=(num_heroes_per_team, total_traits), name='features_team_2')\n",
    "features_combined = Input(shape=(num_heroes_per_team * 2, total_traits), name='features_combined')\n",
    "\n",
    "# Get outputs from GNN branches\n",
    "output_1 = gnn_branch_1([features_team_1, A1])\n",
    "output_2 = gnn_branch_2([features_team_2, A2])\n",
    "output_combined = gnn_combined([features_combined, A_combined])\n",
    "\n",
    "# Combine outputs\n",
    "combined_output = Concatenate()([output_1, output_2, output_combined])\n",
    "\n",
    "# MLP for final processing\n",
    "mlp_layer = Dense(64, activation='relu')(combined_output)\n",
    "final_output = Dense(1, activation='sigmoid')(mlp_layer)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=[features_team_1, features_team_2, features_combined, A1, A2, A_combined], outputs=final_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Split training data to create a validation set\n",
    "validation_split = 0.1\n",
    "val_size = int(len(X_team1_train) * validation_split)\n",
    "X_team1_val, X_team2_val, y_val = X_team1_train[:val_size], X_team2_train[:val_size], y_train[:val_size]\n",
    "X_team1_train, X_team2_train, y_train = X_team1_train[val_size:], X_team2_train[val_size:], y_train[val_size:]\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=4, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_team1_train, X_team2_train, np.concatenate([X_team1_train, X_team2_train], axis=1), A1, A2, A_combined],\n",
    "    y_train,\n",
    "    validation_data=([X_team1_val, X_team2_val, np.concatenate([X_team1_val, X_team2_val], axis=1), A1, A2, A_combined], y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [X_team1_test, X_team2_test, np.concatenate([X_team1_test, X_team2_test], axis=1), A1, A2, A_combined],\n",
    "    y_test\n",
    ")\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Try importing various keras modules to check their existence\n",
    "try:\n",
    "    from tensorflow.keras import models\n",
    "    from tensorflow.keras import layers\n",
    "    print(\"Keras modules are importable.\")\n",
    "except ImportError as e:\n",
    "    print(\"Error importing Keras:\", e)\n",
    "\n",
    "# Specifically check for datasets\n",
    "try:\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    print(\"MNIST module is importable.\")\n",
    "except ImportError as e:\n",
    "    print(\"Error importing MNIST:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Permute, Concatenate, Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming encoded_training_data and all_wins are defined\n",
    "# ...\n",
    "\n",
    "X = np.array(encoded_training_data)\n",
    "y = np.array(all_wins)\n",
    "\n",
    "# Split the data for each team\n",
    "X_team1 = X[:, 0:5, :]\n",
    "X_team2 = X[:, 5:10, :]\n",
    "\n",
    "# Split the data into training and testing sets for each team\n",
    "X_team1_train, X_team1_test, X_team2_train, X_team2_test, y_train, y_test = train_test_split(\n",
    "    X_team1, X_team2, y, test_size=0.15, random_state=3435,# shuffle = True\n",
    ")\n",
    "\n",
    "print('size of x team 1: ', X_team1_train.size)\n",
    "print('size of x team 2: ', X_team2_train.size)\n",
    "print('size of y: ', y_train.size)\n",
    "\n",
    "\n",
    "# Model architecture\n",
    "input_1 = Input(shape=(5, 15))  # Input for team 1\n",
    "input_2 = Input(shape=(5, 15))  # Input for team 2\n",
    "\n",
    "lstm_1 = LSTM(150, return_sequences=True, activation='tanh')(input_1)\n",
    "permute_1 = Permute((2, 1))(lstm_1)\n",
    "\n",
    "lstm_2 = LSTM(150, return_sequences=True, activation='tanh')(input_2)\n",
    "permute_2 = Permute((2, 1))(lstm_2)\n",
    "\n",
    "# Concatenate the outputs from both teams\n",
    "concatenated = Concatenate()([permute_1, permute_2])\n",
    "\n",
    "# Flatten the concatenated features\n",
    "flatten = Flatten()(concatenated)\n",
    "\n",
    "# Additional dense layers as in your original model\n",
    "dense_output = Dense(256, activation='relu')(flatten)\n",
    "dense_output = Dense(128, activation='relu')(dense_output)\n",
    "dense_output = Dense(64, activation='relu')(dense_output)\n",
    "output = Dense(1, activation='sigmoid')(dense_output)\n",
    "\n",
    "model = keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=4,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_team1_train, X_team2_train],\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [X_team1_test, X_team2_test],\n",
    "    y_test\n",
    ")\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary to understand its layers\n",
    "model.summary()\n",
    "\n",
    "# If you have pydot and graphviz installed, you can also plot the model as a graph\n",
    "# This will not execute in this environment but can be run in a local environment with the necessary libraries\n",
    "try:\n",
    "    # Save the model plot to a file\n",
    "    tf.keras.utils.plot_model(model, to_file='/mnt/data/model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    print(\"Model plot saved.\")\n",
    "except ImportError as e:\n",
    "    print(\"pydot or graphviz libraries are not available to plot the model.\")\n",
    "\n",
    "# To display the saved image if it was successful\n",
    "from IPython.display import Image\n",
    "Image(filename='/mnt/data/model_plot.png')\n",
    "\n",
    "\n",
    "# Assuming you have a single match input split into team1_data and team2_data\n",
    "team1_data = X[0:1, 0:5, :]  # Data for the first team of the first match\n",
    "team2_data = X[0:1, 5:10, :] # Data for the second team of the first match\n",
    "\n",
    "# Pass the data through the model to simulate inference\n",
    "output = model.predict([team1_data, team2_data])\n",
    "\n",
    "# Describe the flow through the network\n",
    "flow_description = f\"\"\"\n",
    "The input data for the match is split into two arrays: one for each team.\n",
    "- Team 1 data shape: {team1_data.shape}\n",
    "- Team 2 data shape: {team2_data.shape}\n",
    "\n",
    "This data is then fed into their respective LSTM layers, which process the sequences.\n",
    "The outputs from the LSTMs are then permuted and concatenated to form a single array, which is then flattened.\n",
    "\n",
    "The flattened array is passed through a series of Dense layers with ReLU activations, and finally, a Dense layer with a sigmoid activation function to predict the match outcome.\n",
    "\n",
    "The model output (probability of a win for team 1) is: {output[0][0]}\n",
    "\"\"\"\n",
    "\n",
    "print(flow_description)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW6740",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
